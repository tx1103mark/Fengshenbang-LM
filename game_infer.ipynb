{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tx1103mark/Fengshenbang-LM/blob/main/game_infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glm_merge_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/glm4_9b_dev_result.json'\n",
        "save_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/glm4_9b_dev_result_filter.json'\n",
        "save_out=open(save_path,mode='w')\n",
        "with open(glm_merge_path,mode='r') as f:\n",
        "    for line in f.readlines():\n",
        "      temp=json.loads(line)\n",
        "      temp['answer']=temp['answer'].strip()\n",
        "      save_out.write(json.dumps(temp,ensure_ascii=False)+'\\n')\n",
        "save_out.close()"
      ],
      "metadata": {
        "id": "NcsDrDL_dZ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glm_merge_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/glm4_9b_dev_result_filter.json'\n",
        "import json\n",
        "import pandas as pd\n",
        "data=[]\n",
        "with open(glm_merge_path,mode='r') as f:\n",
        "    for line in f.readlines():\n",
        "      temp=json.loads(line)\n",
        "      data.append(temp)\n",
        "df=pd.DataFrame.from_dict(data)\n",
        "\n",
        "qwen_merge_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/qwen14b_dev.json'\n",
        "qwen_data=[]\n",
        "with open(qwen_merge_path,mode='r') as f:\n",
        "    for line in f.readlines():\n",
        "      temp=json.loads(line)\n",
        "      qwen_data.append(temp)\n",
        "qwen_df=pd.DataFrame.from_dict(qwen_data)\n",
        "\n",
        "\n",
        "dev_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/dev.json'\n",
        "stand_answers=[]\n",
        "with open(dev_path,mode='r') as f:\n",
        "    json_list=json.loads(f.read())\n",
        "    for json_data in json_list:\n",
        "        # print(json_data)\n",
        "        stand_answers.append(json_data['答案'])\n",
        "df['qwen14b-answer'] = qwen_df['answer']\n",
        "df['stand'] = stand_answers\n",
        "\n",
        "df.to_excel('/opt/huawei/explorer-env/algorithm/z00421835_longbench/merge.xlsx',index=False)\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "nMHhLHQ9dpCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process():\n",
        "    glm_merge_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/glm4_9b_merge_222_result_filter.json'\n",
        "    import json\n",
        "    import pandas as pd\n",
        "    data=[]\n",
        "    with open(glm_merge_path,mode='r') as f:\n",
        "        for line in f.readlines():\n",
        "          temp=json.loads(line)\n",
        "          data.append(temp)\n",
        "    df=pd.DataFrame.from_dict(data)\n",
        "\n",
        "\n",
        "    qwen_merge_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/qwen14b_result_filter.json'\n",
        "    qwen_data=[]\n",
        "    with open(qwen_merge_path,mode='r') as f:\n",
        "        for line in f.readlines():\n",
        "          temp=json.loads(line)\n",
        "          qwen_data.append(temp)\n",
        "    qwen_df=pd.DataFrame.from_dict(qwen_data)\n",
        "\n",
        "    other_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/qwen_merge_glm4.json'\n",
        "    other=open(other_path,mode='w')\n",
        "\n",
        "    df['qwen_answer'] = qwen_df['answer']\n",
        "    merge_answers=[]\n",
        "    # print(df.head())\n",
        "    for index, row in df.iterrows():\n",
        "        # print(row)\n",
        "        std_answer=row['answer']\n",
        "        # if '无法回答' in row['qwen_answer']:\n",
        "        #     print(row['ID'])\n",
        "        #     std_answer=row['answer']\n",
        "        if '您' in row['answer'] and '您' not in row['qwen_answer']:\n",
        "            # print('---------',row['ID'])\n",
        "            temp={'ID':row['ID'],'question':row['question'],'answer':row['qwen_answer']}\n",
        "            other.write(json.dumps(temp,ensure_ascii=False)+'\\n')\n",
        "            # pass\n",
        "        # if '您' in row['answer'] and '您' in row['qwen_answer']:\n",
        "        #     print(row)\n",
        "        if '不，' in row['qwen_answer'] and '不，' not in row['answer']:\n",
        "            temp={'ID':row['ID'],'question':row['question'],'answer':row['qwen_answer']}\n",
        "            other.write(json.dumps(temp,ensure_ascii=False)+'\\n')\n",
        "    other.close()\n",
        "\n",
        "post_process()"
      ],
      "metadata": {
        "id": "Y4RMIOnBdwAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "from transformers import set_seed\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "set_seed(42)\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device='cuda'\n",
        "topn=40\n",
        "\n",
        "import os\n",
        "os.environ[\"USE_TF\"] = \"0\"\n",
        "if  os.path.exists('/home/ma-user/work/dataset/'):\n",
        "    model_path='/home/ma-user/work/dataset/z30030640_data/SearchLLM-embedding/bert'\n",
        "else:\n",
        "    model_path='/opt/huawei/dataset/model_dir/z00421835/SearchLLM-embedding/bert'\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(model_path)\n",
        "model.to('cuda')\n",
        "\n",
        "def split_text_into_chunks(text):\n",
        "    # 按换行符分割文本为段落\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    # 初始化结果列表和当前 chunk\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # 去除段落两端的空格\n",
        "        paragraph = paragraph.strip()\n",
        "\n",
        "        # 如果段落为空，则跳过\n",
        "        if not paragraph:\n",
        "            continue\n",
        "\n",
        "        # 检查段落开头的字符串长度\n",
        "        if len(paragraph) < 5:\n",
        "            # 如果当前 chunk 不为空，则将当前 chunk 添加到结果列表\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            # 开始一个新的 chunk\n",
        "            current_chunk = paragraph\n",
        "        else:\n",
        "            # 否则，将内容追加到当前 chunk\n",
        "            current_chunk += \" \" + paragraph\n",
        "\n",
        "    # 添加最后一个 chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def get_top_chunk_index(query,chunks,model,topn):\n",
        "    sentences_1 = [query]\n",
        "    sentences_2 = chunks\n",
        "    embeddings_1 = model.encode(sentences_1, normalize_embeddings=True,batch_size=1)\n",
        "    embeddings_2 = model.encode(sentences_2, normalize_embeddings=True,batch_size=1)\n",
        "\n",
        "    similarity = embeddings_1 @ embeddings_2.T\n",
        "    # print('-----similarity------',similarity)\n",
        "    top_k=topn\n",
        "    if len(chunks) < topn:\n",
        "        top_k=len(chunks)\n",
        "    index=np.argpartition(similarity[0], -top_k)[-top_k:]\n",
        "    return index\n",
        "\n",
        "\n",
        "def filter_item(query,item,topn):\n",
        "    chunks=split_text_into_chunks(item)\n",
        "    index=get_top_chunk_index(item,chunks,model,topn)\n",
        "    filter_chunks=[chunks[i] for i in index]\n",
        "    return '\\n'.join(filter_chunks)\n",
        "\n",
        "def glm4_model_test():\n",
        "       if os.path.exists('/opt/huawei/explorer-env'):\n",
        "           qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/z00421835/model_save/glm4_test/merge-222/'\n",
        "           path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/test-B-0722.json'\n",
        "           out_path='/opt/huawei/explorer-env/dataset/nlp_large_data/z00421835/round1_training_data/glm4-test-B-local.json'\n",
        "       else:\n",
        "           qwen_model_path='/home/ma-user/work/dataset/nlp_large_model_new/z00421835/model_save/glm4_test/merge-74/'\n",
        "           path='/home/ma-user/work/dataset/nlp_large_data/z00421835/round1_training_data/test-B-0722.json'\n",
        "           out_path='/home/ma-user/work/dataset/nlp_large_data/z00421835/round1_training_data/glm4-test-B-local-merge-74.json'\n",
        "\n",
        "       model = AutoModelForCausalLM.from_pretrained(\n",
        "        qwen_model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    ).eval()\n",
        "       tokenizer = AutoTokenizer.from_pretrained(qwen_model_path,trust_remote_code=True)\n",
        "\n",
        "       examples=[]\n",
        "       with open(path,mode='r') as f:\n",
        "         for line in f.readlines():\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "       count = 0\n",
        "       target = 300\n",
        "\n",
        "       with open(out_path,mode='w') as out:\n",
        "           for data in tqdm(examples, total=len(examples)):\n",
        "                count+=1\n",
        "                if count < target:\n",
        "                    continue\n",
        "\n",
        "                name=data['产品名']\n",
        "                item=data['条款']\n",
        "                question=data['问题']\n",
        "\n",
        "                # item=filter_item(question,item,topn)\n",
        "\n",
        "                prompt=f'''## 角色设定\n",
        "            你是一名资深的保险顾问，擅长回答保险产品及其条款的相关问题。\n",
        "\n",
        "            ## 任务背景与描述\n",
        "               你的职责是根据【保险产品名】和【保险条款】来回答【用户问题】,直接给出答案，请注意答案的简洁和精准性。\n",
        "\n",
        "            【保险产品名】：{name}\n",
        "            【保险条款】：{item}\n",
        "            【用户问题】: {question}\n",
        "            '''\n",
        "\n",
        "                inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
        "                                               add_generation_prompt=True,\n",
        "                                               tokenize=True,\n",
        "                                               return_tensors=\"pt\",\n",
        "                                               return_dict=True\n",
        "                                               )\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                gen_kwargs = {\"max_length\": 100000, \"do_sample\": True, \"top_k\": 1}\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "                    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "                    response=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    # answer=response.split('答案：')[-1]\n",
        "                    print(f'---query:{question}----response:{response}---------')\n",
        "                    out_dict={\"ID\":data['ID'],\"question\":question,\"answer\":response}\n",
        "                    out.write(json.dumps(out_dict,ensure_ascii=False)+'\\n')\n",
        "       # out.close()\n",
        "\n",
        "\n",
        "def qwen_model_infer():\n",
        "    if os.path.exists('/opt/huawei/explorer-env/dataset/'):\n",
        "        qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/openLLM/Qwen2-7B-Instruct'\n",
        "        # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/openLLM/Qwen1.5-14B-Chat'\n",
        "        path='round1_training_data/test.json'\n",
        "        out_path='round1_training_data/result.json'\n",
        "    else:\n",
        "        # qwen_model_path='/opt/huawei/dataset/model_dir/openLLM/Qwen1.5-14B-Chat'\n",
        "        # qwen_model_path='/opt/huawei/dataset/model_dir/z00421835/model_save/qwen14b_mtp/merge-74'\n",
        "        qwen_model_path='/opt/huawei/dataset/model_dir/openLLM/Qwen2-7B-Instruct'\n",
        "        path='/opt/huawei/dataset/data_dir/z00421835/round1_training_data/test-B-0722.json'\n",
        "        out_path='/opt/huawei/dataset/data_dir/z00421835/round1_training_data/qwen14b_testB_result.json'\n",
        "    # qwen_model_path='/home/ma-user/work/dataset/nlp_large_model_new/openLLM/Qwen2-7B-Instruct'\n",
        "    # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/ywx1272703/model_eval/Qwen2-7B-full_v1_11_ckpt858'\n",
        "    # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/L1_models/Qwen2-7B-lora/l1/merge-1200/'\n",
        "\n",
        "    # qwen_model_path='/opt/huawei/dataset/model_dir/openLLM/Qwen2-72B-Instruct'\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        qwen_model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    ).eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(qwen_model_path)\n",
        "\n",
        "    examples=[]\n",
        "    with open(path,mode='r') as f:\n",
        "         for line in f.readlines():\n",
        "            examples.append(json.loads(line))\n",
        "    with open(out_path,mode='w') as out:\n",
        "        for data in tqdm(examples, total=len(examples)):\n",
        "\n",
        "            name=data['产品名']\n",
        "            item=data['条款']\n",
        "            question=data['问题']\n",
        "\n",
        "            item=filter_item(question,item,topn)\n",
        "\n",
        "            prompt=f'''## 角色设定\n",
        "        你是一名资深的保险顾问，擅长回答保险产品及其条款的相关问题。\n",
        "\n",
        "        ## 任务背景与描述\n",
        "          你的职责是根据【保险产品名】和【保险条款】来回答【用户问题】,直接给出答案，请注意答案的简洁和精准性。\n",
        "\n",
        "        【保险产品名】：{name}\n",
        "        【保险条款】：{item}\n",
        "        【用户问题】: {question}\n",
        "        '''\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            text = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            # print(text)\n",
        "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "            # Directly use generate() and tokenizer.decode() to get the output.\n",
        "            # Use `max_new_tokens` to control the maximum output length.\n",
        "            generated_ids = model.generate(\n",
        "                model_inputs.input_ids,\n",
        "                max_new_tokens=512\n",
        "            )\n",
        "            generated_ids = [\n",
        "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "            ]\n",
        "\n",
        "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "            # answer=response.split('答案：')[-1]\n",
        "            print(f'---query:{question}----response:{response}---------')\n",
        "            out_dict={\"ID\":data['ID'],\"question\":question,\"answer\":response}\n",
        "            out.write(json.dumps(out_dict,ensure_ascii=False)+'\\n')\n",
        "\n",
        "def glm4_model_infer():\n",
        "    if os.path.exists('/opt/huawei/explorer-env/dataset/'):\n",
        "        qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/openLLM/glm-4-9b-chat'\n",
        "        # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/openLLM/Qwen1.5-14B-Chat'\n",
        "        path='round1_training_data/test.json'\n",
        "        out_path='round1_training_data/result.json'\n",
        "    else:\n",
        "        # qwen_model_path='/opt/huawei/dataset/model_dir/openLLM/glm-4-9b-chat'\n",
        "        qwen_model_path='/opt/huawei/dataset/model_dir/z00421835/model_save/glm4_test/merge-222/'\n",
        "        qwen_model_path='/opt/huawei/dataset/model_dir/z00421835/model_save/glm4_test/merge-74/'\n",
        "        path='/opt/huawei/dataset/data_dir/z00421835/round1_training_data/test-B-0722.json'\n",
        "        out_path=f'/opt/huawei/dataset/data_dir/z00421835/round1_training_data/glm4_9b_testB_result_merge_74_0.json'\n",
        "    # qwen_model_path='/home/ma-user/work/dataset/nlp_large_model_new/openLLM/Qwen2-7B-Instruct'\n",
        "    # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/ywx1272703/model_eval/Qwen2-7B-full_v1_11_ckpt858'\n",
        "    # qwen_model_path='/opt/huawei/explorer-env/dataset/nlp_large_model_new/L1_models/Qwen2-7B-lora/l1/merge-1200/'\n",
        "\n",
        "    # qwen_model_path='/opt/huawei/dataset/model_dir/openLLM/Qwen2-72B-Instruct'\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        qwen_model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\"\n",
        "    ).eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(qwen_model_path,trust_remote_code=True)\n",
        "\n",
        "    # out=open(out_path,mode='w')\n",
        "\n",
        "    # with open(path,mode='r') as f:\n",
        "    #      examples=json.loads(f.read())\n",
        "\n",
        "    examples=[]\n",
        "    with open(path,mode='r') as f:\n",
        "         for line in f.readlines():\n",
        "            examples.append(json.loads(line))\n",
        "    # count=400\n",
        "    count=102\n",
        "    cur=0\n",
        "    with open(out_path,mode='w') as out:\n",
        "        for data in tqdm(examples, total=len(examples)):\n",
        "            cur +=1\n",
        "            if cur < count:\n",
        "                continue\n",
        "            # if cur > 600:\n",
        "            #     break\n",
        "            # print('------cur------------',cur)\n",
        "            name=data['产品名']\n",
        "            item=data['条款']\n",
        "            question=data['问题']\n",
        "\n",
        "            # item=filter_item(question,item,topn)\n",
        "\n",
        "            prompt=f'''## 角色设定\n",
        "        你是一名资深的保险顾问，擅长回答保险产品及其条款的相关问题。\n",
        "\n",
        "        ## 任务背景与描述\n",
        "          你的职责是根据【保险产品名】和【保险条款】来回答【用户问题】,直接给出答案，请注意答案的简洁和精准性。\n",
        "\n",
        "        【保险产品名】：{name}\n",
        "        【保险条款】：{item}\n",
        "        【用户问题】: {question}\n",
        "        '''\n",
        "\n",
        "            inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
        "                                           add_generation_prompt=True,\n",
        "                                           tokenize=True,\n",
        "                                           return_tensors=\"pt\",\n",
        "                                           return_dict=True\n",
        "                                           )\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            gen_kwargs = {\"max_length\": 128000, \"do_sample\": True, \"top_k\": 1}\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, **gen_kwargs)\n",
        "                outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
        "                response=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                # answer=response.split('答案：')[-1]\n",
        "                # print(f'---query:{question}----response:{response}---------')\n",
        "                out_dict={\"ID\":data['ID'],\"question\":question,\"answer\":response}\n",
        "                out.write(json.dumps(out_dict,ensure_ascii=False)+'\\n')\n",
        "    # out.close()\n",
        "# qwen_model_infer()\n",
        "glm4_model_infer()\n",
        "# glm4_model_test()\n",
        ""
      ],
      "metadata": {
        "id": "W4myAOQFeW1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "#https://github.com/modelscope/swift/blob/57ea4497c0e81c4ef7d9a6bb275366db0a12634e/docs/source/LLM/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0.md\n",
        "#https://github.com/modelscope/swift/blob/57ea4497c0e81c4ef7d9a6bb275366db0a12634e/docs/source/LLM/%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%8B%93%E5%B1%95.md#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86\n",
        "\n",
        "echo \"开始安装环境\"\n",
        "date\n",
        "cp /opt/huawei/dataset/model_dir/z00421835/envs/swift.tar.gz /cache/\n",
        "tar -xzf /cache/swift.tar.gz -C /cache/\n",
        "date\n",
        "echo \"结束安装环境\"\n",
        "\n",
        "source /cache/bin/activate\n",
        "\n",
        "# pip list\n",
        "# pip list |grep transformers\n",
        "pip install -U transformers -i http://pip.modelarts.private.com/repository/pypi/simple/ --trusted-host pip.modelarts.private.com\n",
        "\n",
        "pip install sentence-transformers==2.7.0 -i http://pip.modelarts.private.com/repository/pypi/simple/ --trusted-host pip.modelarts.private.com\n",
        "\n",
        "python qwen2-game-infer.py\n"
      ],
      "metadata": {
        "id": "hagEZRCVeifY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}